{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apparent-laser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (0.18.3)\n",
      "Requirement already satisfied: openai in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (1.39.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: cached-property in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from openai) (1.5.2)\n",
      "Requirement already satisfied: sniffio in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: tqdm>4 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: idna>=2.8 in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: exceptiongroup in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (2020.12.5)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: importlib-metadata in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (3.7.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (0.5.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: setuptools in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from wandb) (49.6.0.post20210108)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (2.15.0)\n",
      "Requirement already satisfied: pyyaml in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.12.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (4.24.4)\n",
      "Requirement already satisfied: platformdirs in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (4.0.0)\n",
      "Requirement already satisfied: setproctitle in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /fs01/home/yuxuanli/.local/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /fs01/pkgs/jupyterhub/lib/python3.7/site-packages (from importlib-metadata->pydantic<3,>=1.9.0->openai) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "laden-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  6 23:28:38 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:D9:00.0 Off |                    0 |\r\n",
      "| N/A   34C    P8    15W /  70W |      0MiB / 15360MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "musical-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chat-0ffad07e4b6e4d15b3fb87a263916dcc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1728271781, model='Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=19, total_tokens=42))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# The url is located in the .vLLM_model-variant_url file in the corresponding model directory.\n",
    "client = OpenAI(base_url=\"http://gpu051:8080/v1\", api_key=\"EMPTY\")\n",
    "\n",
    "# Update the model path accordingly\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "previous-switzerland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulation-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "endless-manhattan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriuken\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "colored-pound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handmade-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple tabular TAMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "representative-accountability",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-07053f55539f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Run the TAMER training for 10 episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain_tamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Display the learned Q-table from human feedback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-07053f55539f>\u001b[0m in \u001b[0;36mtrain_tamer\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Simulate feedback for the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Update the Q-table using the human feedback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-6c6cc88725d3>\u001b[0m in \u001b[0;36mget_feedback\u001b[0;34m(client, content, model, prompt)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Update the model path accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         messages=[\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "env_name = \"CliffWalking-v0\"\n",
    "wandb_project_name = \"LanguageFeedback\" + env_name + \"Pilot\"\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Parameters for the learning algorithm\n",
    "epsilon = 0.1  # Epsilon greedy \n",
    "alpha = 0.5    # Learning rate\n",
    "gamma = 0.99   # Discount factor\n",
    "max_episode_steps = 100\n",
    "\n",
    "# Action selection using epsilon-greedy policy\n",
    "def select_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(n_actions)) \n",
    "    else:\n",
    "        return np.argmax(Q_table[state])  \n",
    "\n",
    "# Train the agent using human feedback\n",
    "def train_tamer(episodes):\n",
    "    for episode in range(episodes):\n",
    "        episodic_reward = 0\n",
    "        state, _ = env.reset()  # Reset environment at the beginning of each episode\n",
    "        done = False\n",
    "        episode_steps = 0\n",
    "        while not done and episode_steps < max_episode_steps:\n",
    "            action = select_action(state)  \n",
    "            next_state, reward, done, truncated, _ = env.step(action)  \n",
    "            \n",
    "            episodic_reward += reward\n",
    "            episode_steps += 1\n",
    "            \n",
    "            # Simulate feedback for the action\n",
    "            feedback = get_feedback(state, action)\n",
    "            \n",
    "            # Update the Q-table using the human feedback\n",
    "            Q_table[state, action] += alpha * (feedback - Q_table[state, action])\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Optional: print progress\n",
    "            print(f\"Episode {episode + 1}, State: {state}, Action: {action}, Feedback: {feedback}\")\n",
    "        \n",
    "        wandb.log({\"episodic_reward\": episodic_reward}, step=episode)\n",
    "        \n",
    "        \n",
    "\n",
    "# Run the TAMER training for 10 episodes\n",
    "train_tamer(10)\n",
    "\n",
    "# Display the learned Q-table from human feedback\n",
    "print(\"Learned Q-table from human feedback:\")\n",
    "print(Q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "worth-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(episodes, env_name, max_steps = 100):\n",
    "    \"\"\"Evaluate the agent's performance after training.\"\"\"\n",
    "    total_steps = 0\n",
    "    total_rewards = 0\n",
    "    success_count = 0  # Count how many times the agent reaches the goal\n",
    "    cliff_falls = 0    # Count how many times the agent falls off the cliff\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()  # Reset environment at the start of each episode\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # Choose the best action (greedy policy) based on the learned Q-table\n",
    "            action = np.argmax(Q_table[state])\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Accumulate reward and step count\n",
    "            episode_rewards += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Check if agent falls off the cliff (in CliffWalking, reward is -100 for the cliff)\n",
    "            if reward == -100:\n",
    "                cliff_falls += 1\n",
    "                break  # Episode ends if agent falls off the cliff\n",
    "            \n",
    "            # Check if agent reaches the goal (state 47)\n",
    "            if state == 47:\n",
    "                success_count += 1\n",
    "                break  # Episode ends when the agent reaches the goal\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "        \n",
    "        total_rewards += episode_rewards\n",
    "        total_steps += steps\n",
    "\n",
    "        print(f\"Episode {episode+1}: Steps = {steps}, Rewards = {episode_rewards}\")\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_steps = total_steps / episodes\n",
    "    avg_rewards = total_rewards / episodes\n",
    "    success_rate = success_count / episodes * 100\n",
    "    cliff_fall_rate = cliff_falls / episodes * 100\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Average steps per episode: {avg_steps:.2f}\")\n",
    "    print(f\"Average rewards per episode: {avg_rewards:.2f}\")\n",
    "    print(f\"Success rate: {success_rate:.2f}%\")\n",
    "    print(f\"Cliff fall rate: {cliff_fall_rate:.2f}%\")\n",
    "    \n",
    "    return avg_steps, avg_rewards, success_rate, cliff_fall_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "alert-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chat-9be75e0edac5456396281755f06d59e6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Let\\'s think step by step.\\n\\nGiven the state [3,0], we are at the starting location.\\n\\nThe action is \"move right\", assuming the original coordinate is [X,Y], the result after move right is a new coordinate [X, Y+1].\\r\\n\\r\\nSo, if we move right from [3,0], we get [3,1].\\r\\n\\r\\nThis is a valid move because we are not moving into the cliff (which is at [3,1..10]) and we are not moving out of the grid boundaries.\\r\\n\\r\\nTherefore, the action \"move right\" is: GOOD', role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1728338497, model='Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=122, prompt_tokens=336, total_tokens=458))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Let\\'s think step by step.\\n\\nGiven the state [3,0], we are at the starting location.\\n\\nThe action is \"move right\", assuming the original coordinate is [X,Y], the result after move right is a new coordinate [X, Y+1].\\r\\n\\r\\nSo, if we move right from [3,0], we get [3,1].\\r\\n\\r\\nThis is a valid move because we are not moving into the cliff (which is at [3,1..10]) and we are not moving out of the grid boundaries.\\r\\n\\r\\nTherefore, the action \"move right\" is: GOOD'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# The url is located in the .vLLM_model-variant_url file in the corresponding model directory.\n",
    "feedback_client = OpenAI(base_url=\"http://gpu001:8080/v1\", api_key=\"EMPTY\")\n",
    "binary_feedback_prompt = prompt_construct_binary_feedback_cliff_walking(36, 1)\n",
    "completion = get_feedback(client=feedback_client, content=CLIFFWALKING_PROMPT_KNOWN_DYNAMICS_COT, model=\"Meta-Llama-3.1-8B-Instruct\", prompt=binary_feedback_prompt)\n",
    "print(completion)\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "secondary-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "annual-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(client, content:str=\"\", model=\"Meta-Llama-3.1-8B-Instruct\", prompt:str=\"\"):\n",
    "    \"\"\"\n",
    "    state, action, url_in_vec_inf\n",
    "    \"\"\"\n",
    "    # Update the model path accordingly\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "working-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIFFWALKING_PROMT_UNKNOWN_DYNAMICS_COT = \"We are dealing with the cliff walking problem.\\r\\nBelow is the problem description:\\r\\nCliff walking involves crossing a gridworld from start to goal while avoiding falling off a cliff.\\r\\nThe game starts with the player at location [3, 0] of the 4x12 grid world with the goal located at [3, 11]. If the player reaches the goal the episode ends. A cliff runs along [3, 1..10]. If the player moves to a cliff location it returns to the start location. The player makes moves until they reach the goal.\\r\\nThe possible actions are:\\r\\n1. move up\\r\\n2. move down\\r\\n3. move left\\r\\n4. move right\\r\\nYou will be answering the question following this template:\\r\\nGiven this state: <STATE>  \\r\\nGiven the action: <ACTION>\\r\\nIs this action good or bad? Let\\'s think step by step.\\r\\nThe action is: <GOOD_OR_BAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "chemical-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIFFWALKING_PROMPT_KNOWN_DYNAMICS_COT = \"We are dealing with the cliff walking problem.\\r\\nBelow is the problem description:\\r\\nCliff walking involves crossing a gridworld from start to goal while avoiding falling off a cliff.\\r\\nThe game starts with the player at location [3, 0] of the 4x12 grid world with the goal located at [3, 11]. If the player reaches the goal the episode ends. A cliff runs along [3, 1..10]. If the player moves to a cliff location it returns to the start location. The player makes moves until they reach the goal.\\r\\nThe possible actions are:\\r\\n1. move up, assuming the original coordinate is [X,Y], the result after move up is a new coordinate [X-1, Y]\\r\\n2. move down, assuming the original coordinate is [X,Y], the result after move down is a new coordinate [X+1, Y]\\r\\n3. move left, assuming the original coordinate is [X,Y], the result after move up is a new coordinate [X, Y-1]\\r\\n4. move right, assuming the original coordinate is [X,Y], the result after move up is a new coordinate [X, Y+1]\\r\\nYou will be answering the question following this template:\\r\\nGiven this state: <STATE>  \\r\\nGiven the action: <ACTION>\\r\\nIs this action good or bad? Let\\'s think step by step.\\r\\nThe action is: <GOOD_OR_BAD>\\r\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "major-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_construct_binary_feedback_cliff_walking(state, action):\n",
    "    \"\"\"\n",
    "    Given this state: <STATE>  \\r\\nGiven the action: <ACTION>\\r\\nIs this action good or bad? Let\\'s think step by step.\\r\\nThe action is: <GOOD_OR_BAD>\\r\\n\n",
    "    \"\"\"\n",
    "    state_coordinates = cliff_walking_state_to_coordinates(state)\n",
    "    state_str = \"[\" + str(state_coordinates[0]) + \",\" + str(state_coordinates[1]) +\"]\"\n",
    "    action_list = [\"move up\", \"move right\", \"move down\", \"move left\"]\n",
    "    final_prompt = \"Given this state: \" + state_str +   \"\\r\\nGiven the action: \" + action_list[action] + \"\\r\\nIs this action good or bad? Let\\'s think step by step.\\r\\nThe action is: \"\n",
    "    return final_prompt\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "electoral-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pediatric-prize",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(48)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disciplinary-night",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "willing-audit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "warming-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cliff_walking_state_to_coordinates(state):\n",
    "    return np.asarray([int(state/12), int(state%12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "varying-stand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliff_walking_state_to_coordinates(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "filled-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Given this state: [2,7]\\r\\nGiven the action: move up\\r\\nIs this action good or bad? Let's think step by step.\\r\\nThe action is: \""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_construct_binary_feedback_cliff_walking(31, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-southeast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
